\documentclass[12pt]{article}

\usepackage{geometry}
\usepackage[hidelinks, bookmarks = true]{hyperref}
\usepackage[numbered]{bookmark}
\usepackage{booktabs}
\usepackage{float}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{titletoc}
\usepackage{indentfirst}
\usepackage{fancyhdr} 
\usepackage{longtable}
\usepackage{supertabular}
\usepackage[normalem]{ulem}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{xurl}
\usepackage{tikz}
\usepackage{lastpage}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{caption}
\usepackage{threeparttable}

\geometry{a4paper, left = 1.6cm, right = 1.6cm, top = 2cm, bottom = 2cm, includehead}
\captionsetup[table]{font={small}}

\pagestyle{fancy}
\fancyhead[R]{\thepage /\pageref{LastPage}}
\fancyhead[L]{GROUP 28}
\fancyfoot{}
\fancyfoot{}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}
\setlength{\headheight}{14.49998pt}
\addtolength{\topmargin}{-2.49998pt}

\begin{document}
\noindent\rule{\textwidth}{1pt}
\begin{center}
    \LARGE \textbf{Assignment 1: Recommender Systems}
\end{center}
\noindent\rule{\textwidth}{0.5pt}
\begin{center}
    \textbf{Group 28}\par
    \vspace{0.3cm}
Shuang Fan\phantom{space}Kaiteng Jiang\phantom{space}Shupei Li\\
s3505847\phantom{spacespac}sxxxxxxx\phantom{spacespa}s3430863
\end{center}
\section{Recommender systems}
RMSE and MAE are chosen as metrics in Task 1.1-1.3. The predicted value in all task is a real number that may exceed the valid range of rating. To fix this problem, we truncate the predicted value according to the function,
\begin{align*}
    f(\hat{x}) &= 
    \begin{cases}
        1,\qquad &\hat{x} < 1,\\
        \hat{x},\qquad &1\leq \hat{x} \leq 5,\\
        5, \qquad &\hat{x} > 5.
    \end{cases}
\end{align*}
\par
It is worth noting that we set the random seed to 1 in all models that involve randomness, e.g. weights initialization, five-fold division, etc. The fixed random state ensures reproducibility.

\subsection{Naive Approaches}
\subsubsection{Experimental Setup}
During the sampling process of an "average rating" recommender, some users or some movies might disappear from the training sets. To fix this problem, we need to define a fall-back value. In our experiments, we build models of global average, user average, movies average and a linear combination of user and movie averages (with and without the intercept). A linear regression model can be expressed as,
\begin{align*}
    pred&=\alpha\cdot avg_{user} + \beta\cdot avg_{movie} + \gamma
\end{align*}
where $\gamma$ is the intercept. After simplifing the variant $\gamma$, we get,
\begin{align*}
    pred&=\alpha\cdot avg_{user} + \beta\cdot avg_{movie}
\end{align*}
\par
Considering the matrix $\mathbf{U}$ with UserID and MovieID, the global average should be the average value of all existed Ratings. For user average rating, we calculate every user's average rating. This is the $avg_{user}$ term in the equation. For movie average rating, we calculate every movie's average rating. This is the $avg_{movie}$ term in the equation. When implementing the linear regression algorithm, we calculate the coefficient and consider the situations that are with and without the intercept $\gamma$.\par
All experiments of Task 1.1 are run on a multi-core CPU Intel(R) Core(TM) i9-9880H CPU @ 2.30GHz.
\subsubsection{Results}
Table \ref{tab:1.1-results} reports RMSE, MAE, and the actual run time of global average, user average, movies average and a linear combination of user and movie averages(with and without the intercept). RMSE and MAE are the mean values of five folds.
\begin{table}[ht]
    \centering
    \caption{Results of Task 1.1}
    \label{tab:1.1-results}
    \begin{threeparttable}
    \begin{tabular}{lllllllll}
        \toprule
        \textbf{Algorithm}  & \textbf{Train RMSE} & \textbf{Train MAE} & \textbf{Test RMSE} & \textbf{Test MAE} & \textbf{Time}\\
        \midrule
        GlobalAvg & 1.1171 & 0.9339 & 1.1171 & 0.9339 & 0.93s\\
        UserAvg & 1.0277 & 0.8227 & 1.0355 & 0.8290 & 1.38s\\
        MovieAvg & 0.9742 & 0.7783 & 0.9794 & 0.7823 & 1.43s\\
        LinearReg & \textbf{0.9145} & \textbf{0.7248} & \textbf{0.9002} & \textbf{0.7122} & 13m 57s\\
        LinearRegNI & 0.9465 & 0.7586 & 0.9345 & 0.7487 & 13m 45s\\
        \bottomrule
    \end{tabular}
    \end{threeparttable}
\end{table}

\subsubsection{Algorithm Analysis}

\subsection{UV Matrix Decomposition}

\subsection{Matrix Factorization}
\subsubsection{Experimental Set-up}
Matrix factorization algorithm in \texttt{gravity-Tikk.pdf} consists of three stages --- initialization, gradient descent, and evaluation. In the experiments, we initialize feature matrices $\mathbf{U}_{I\times K}$ and $\mathbf{M}_{K\times J}$ from a Gaussian distribution $N\sim (0, 0.1)$, where $I$, $J$, and $K$ are maximal UserID, maximal MovieID, and the number of features respectively. \texttt{gravity-Tikk.pdf} combines gradient descent and regularization strategies. Main update formulas are,
\begin{align*}
    u_{ik}^{(t+1)} &= u_{ik}^{(t)} + \eta\cdot\left( 2e_{ij}\cdot m_{kj}^{(t)} - \lambda\cdot u_{ik}^{(t)} \right) \\
    m_{kj}^{(t+1)} &= m_{kj}^{(t)} + \eta\cdot\left( 2e_{ij}\cdot u_{ik}^{(t)} - \lambda \cdot m_{kj}^{(t)} \right) 
\end{align*}
where $\eta$, $\lambda$ are hyperparameters, and $t$ represents the $t$ th iteration. To enhance the efficiency of program, we update the weights based on the rows or columns of matrices, that is,
\begin{align*}
    U^{(t+1)}[i, :] &= U^{(t)}[i, :] + \eta\left( 2e_{ij}M^{(t)}[:, j] - \lambda U^{(t)}[i, :] \right) \\
    M^{(t+1)}[:, j] &= M^{(t)}[:, j] + \eta\left( 2e_{ij}U^{(t)}[i, :] - \lambda M^{(t)}[:, j] \right) 
\end{align*}
Termination condition is achieving the maximum iteration specified by user.\par
We try five different sets of hyperparameters to improve the model performance. Table \ref{tab:1.3-hyper} summarizes all hyperparameters in matrix factorization algorithms. All experiments of Task 1.3 are run on a multi-core CPU Intel(R) Core(TM) i7-10875H CPU @ 2.30GHz. We adopt multiprocessing programming to speed up the program.
\begin{table}[ht]
    \centering
    \caption{Hyperparameters in Matrix Factorization}
    \label{tab:1.3-hyper}
    \begin{tabular}{ll}
        \toprule
        \textbf{Hyperparameter} & \textbf{Meaning}\\
        \midrule
        seeds & The random seed. Default: 1.\\
        num\_factors ($K$) & The number of features.\\
        num\_iter ($N$) & The maximum iteration.\\
        regularization ($\lambda$) & Regularization rate.\\
        learn\_rate ($\eta$) & Learning rate.\\
        \bottomrule
    \end{tabular}
\end{table}

\subsubsection{Results}
Table \ref{tab:1.3-results} reports RMSE, MAE, and the actual run time of matrix factorization algorithm on MovieLens 1M data, with different hyperparameter settings. RMSE and MAE are the mean values of five folds.
\begin{table}[ht]
    \centering
    \caption{Results of Task 1.3}
    \label{tab:1.3-results}
    \begin{threeparttable}
    \begin{tabular}{lllllllll}
        \toprule
        \textbf{$K$} & \textbf{$N$} & \textbf{$\lambda$} & \textbf{$\eta$} & \textbf{Train RMSE} & \textbf{Train MAE} & \textbf{Test RMSE} & \textbf{Test MAE} & \textbf{Time}\\
        \midrule
        *10 & 75 & 0.05 & 0.005 & 0.7689 & 0.6036 & 0.8686 & 0.6785 & 18m 37s\\
        20 & 75 & 0.05 & 0.005 & \textbf{0.7003} & \textbf{0.5475} & 0.8848 & 0.6878 & 18m 38s\\
        10 & 100 & 0.05 & 0.005 & 0.7673 & 0.6020 & 0.8670 & 0.6793 & 24m 38s\\
        10 & 75 & 0.01 & 0.005 & 0.7627 & 0.5949 & 0.8807 & 0.6829 & 18m 29s\\
        10 & 75 & 0.05 & 0.001 & 0.7980 & 0.6292 & \textbf{0.8611} & \textbf{0.6763} & 18m 34s\\
        \bottomrule
    \end{tabular}
    \begin{tablenotes}
        \footnotesize
        \item[*] This set of hyperparameters is suggested by Task 1.3.
    \end{tablenotes}
    \end{threeparttable}
\end{table}
\par
According to Table \ref{tab:1.3-results}, the suggested setting is not the optimal choice.

\subsubsection{Algorithm Analysis}
\noindent\textbf{Notations}\\
As stated in the question,
\begin{itemize}
    \item $M$: The number of movies.
    \item $U$: The number of users.
    \item $R$: The number of ratings.
\end{itemize}
The number of features $K$ is an integer specified by user. We regard $K$ as a constant in the following analysis.\par
\vspace{0.3cm}
\noindent\textbf{Time Complexity}\\
The time complexity of initializing $U$ and $M$ depends on the implementation of random number generation algorithm. For simplicity, we assume the time complexity of the initialization stage is $O(1)$. According to our Python implementation, the \texttt{for} loop to update weights has the time complexity $O(R)$. In the \texttt{for} loop, calculating error, computing gradients, and updating weights all have the time complexity $O(K) \rightarrow O(1)$. To evaluate the performance, we need to traverse the rating table, which leads to $O(R)$ time complexity. Calculating RMSE and MAE also has time complexity $O(R)$. Therefore, the time complexity of our implementation is $O(R)$.\par 
\vspace{0.3cm}
\noindent\textbf{Memory Complexity}\\
We need to initialize $U$, $M$, and rating table at the beginning of the algorithm. This step requires $O(UK + MK + R)\rightarrow O(U + M + R)$ memory. In the \texttt{for} loop, storing error value and gradients needs $O(1)$ memory. During the evaluation, storing predicted values requires $O(R)$ memory and storing metrics needs $O(1)$ memory. Therefore, the memory complexity of our implementation is $O(U + M + R)$.

\subsection{Comparison of Algorithms}
Table \ref{tab:1.4} compares the performance of previously mentioned algorithms, which only includes the model with the best performance on test data if multiple hyperparameters have been explored.
\begin{table}[ht]
    \centering
    \caption{Algorithm Comparison}
    \label{tab:1.4}
    \begin{tabular}{llllll}
        \toprule
        \textbf{Algorithm} & \textbf{Train RMSE} & \textbf{Train MAE} & \textbf{Test RMSE} & \textbf{Test MAE} & \textbf{Time}\\
        \midrule
        GlobalAvg & 1.1171 & 0.9339 & 1.1171 & 0.9339 & 0.93s\\
        UserAvg & 1.0277 & 0.8227 & 1.0355 & 0.8290 & 1.38s\\
        MovieAvg & 0.9742 & 0.7783 & 0.9794 & 0.7823 & 1.43s\\
        LinearReg & 0.9145 & 0.7248 & 0.9002 & 0.7122 & 13m 57s\\
        LinearRegNI & 0.9465 & 0.7586 & 0.9345 & 0.7487 & 13m 45s\\
        Matrix Factorization & 0.7980 & 0.6292 & 0.8611 & 0.6763 & 18m 34s\\
        \bottomrule
    \end{tabular}
\end{table}
\section{Data visualization}

\end{document}
